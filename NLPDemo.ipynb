{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makwanadeepam/30days/blob/master/NLPDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "50Sl9bylsv06",
        "outputId": "f31223cb-1cdb-497b-ff47-fdab73c1f476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (run these in Colab/Terminal first if not installed)\n",
        "# !pip install nltk spacy textblob\n",
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import spacy"
      ],
      "metadata": {
        "id": "JXaTKSr0tns-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKuUgpQ1twkJ",
        "outputId": "bb857de2-f277-4f92-ce46-27c28f546cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love playing football. It makes me very happy, but sometimes I feel tired.\""
      ],
      "metadata": {
        "id": "4vhZ5aZot3_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. TOKENIZE words (basic splitting)"
      ],
      "metadata": {
        "id": "m8y6PmiMvHgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"WORD TOKENIZATION:\")\n",
        "words = text.replace('.', '').replace(',', '').split()\n",
        "print(words)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUXBL7Zxu_Gc",
        "outputId": "f107b80b-faf8-4633-c132-b9258519169d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD TOKENIZATION:\n",
            "['I', 'love', 'playing', 'football', 'It', 'makes', 'me', 'very', 'happy', 'but', 'sometimes', 'I', 'feel', 'tired']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TOKENIZE sentences (split by '.')"
      ],
      "metadata": {
        "id": "Yk4h3V3LvNMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SENTENCE TOKENIZATION:\")\n",
        "sentences = [s.strip() for s in text.split('.') if s.strip() != \"\"]\n",
        "print(sentences)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB7W1UK9vD_x",
        "outputId": "7bc9f794-dbe6-45c8-90ca-556a47dcb018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE TOKENIZATION:\n",
            "['I love playing football', 'It makes me very happy, but sometimes I feel tired']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. TOKENIZE text with stopwords as delimiters"
      ],
      "metadata": {
        "id": "7WBbOJFrvXRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['is', 'am', 'are', 'the', 'a', 'an', 'and', 'but', 'or', 'it', 'me']\n",
        "filtered_words = [w for w in words if w.lower() not in stopwords]\n",
        "print(\"TEXT WITHOUT STOPWORDS:\")\n",
        "print(filtered_words)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Lvt4RUvK0y",
        "outputId": "2ae73faf-bd06-4c08-fd4f-8cab27baf0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT WITHOUT STOPWORDS:\n",
            "['I', 'love', 'playing', 'football', 'makes', 'very', 'happy', 'sometimes', 'I', 'feel', 'tired']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ADD custom stopwords"
      ],
      "metadata": {
        "id": "_YQWOdBevYMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords = stopwords + ['very', 'sometimes']\n",
        "filtered_custom = [w for w in words if w.lower() not in custom_stopwords]\n",
        "print(\"TEXT WITHOUT CUSTOM STOPWORDS:\")\n",
        "print(filtered_custom)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7GzOtlxvait",
        "outputId": "4633c67d-fed7-4bca-94ce-c597c1e2636e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT WITHOUT CUSTOM STOPWORDS:\n",
            "['I', 'love', 'playing', 'football', 'makes', 'happy', 'I', 'feel', 'tired']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. STEMMING (rule-based simple example)"
      ],
      "metadata": {
        "id": "3MWYV3DvvfXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_stem(word):\n",
        "    suffixes = ['ing', 'ed', 's']\n",
        "    for suf in suffixes:\n",
        "        if word.endswith(suf) and len(word) > len(suf) + 1:\n",
        "            return word[:-len(suf)]\n",
        "    return word\n",
        "\n",
        "stemmed_words = [simple_stem(w) for w in filtered_custom]\n",
        "print(\"STEMMED WORDS:\")\n",
        "print(stemmed_words)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td7HEl2Xvhd7",
        "outputId": "21dbcac9-c21d-4dca-9fcc-8485b0ce17d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEMMED WORDS:\n",
            "['I', 'love', 'play', 'football', 'make', 'happy', 'I', 'feel', 'tir']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. LEMMATIZATION (simple dictionary-based)"
      ],
      "metadata": {
        "id": "7oogERnHvmFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_dict = {\n",
        "    'playing': 'play',\n",
        "    'played': 'play',\n",
        "    'makes': 'make',\n",
        "    'tired': 'tire',\n",
        "    'loved': 'love'\n",
        "}\n",
        "\n",
        "lemmatized_words = [lemma_dict.get(w.lower(), w) for w in stemmed_words]\n",
        "print(\"LEMMATIZED WORDS:\")\n",
        "print(lemmatized_words)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbOojxtYvpEX",
        "outputId": "4cb14681-c993-48df-87b3-35b1035361d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEMMATIZED WORDS:\n",
            "['I', 'love', 'play', 'football', 'make', 'happy', 'I', 'feel', 'tir']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. SIMPLE SENTIMENT CLASSIFICATION"
      ],
      "metadata": {
        "id": "gXd9dKzBvrmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# SIMPLE SENTIMENT CLASSIFICATION (USER INPUT VERSION)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# Take user input\n",
        "text = input(\"Enter a sentence for sentiment analysis: \")\n",
        "\n",
        "# Tokenize (basic split)\n",
        "words = text.replace('.', '').replace(',', '').split()\n",
        "\n",
        "# Define word lists\n",
        "positive_words = ['love', 'happy', 'amazing', 'good', 'great', 'wonderful', 'like', 'enjoy']\n",
        "negative_words = ['sad', 'bad', 'tired', 'angry', 'hate', 'boring', 'terrible', 'upset']\n",
        "\n",
        "# Count positive and negative words\n",
        "pos_count = sum(w.lower() in positive_words for w in words)\n",
        "neg_count = sum(w.lower() in negative_words for w in words)\n",
        "\n",
        "# Determine sentiment\n",
        "if pos_count > neg_count:\n",
        "    sentiment = \"Positive üòä\"\n",
        "elif neg_count > pos_count:\n",
        "    sentiment = \"Negative üòî\"\n",
        "else:\n",
        "    sentiment = \"Neutral üòê\"\n",
        "\n",
        "# Print result\n",
        "print(\"\\n--- SENTIMENT CLASSIFICATION RESULT ---\")\n",
        "print(f\"Input Text: {text}\")\n",
        "print(f\"Positive words found: {pos_count}\")\n",
        "print(f\"Negative words found: {neg_count}\")\n",
        "print(f\"Overall Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CRQ7lLIvsOL",
        "outputId": "88f0e4d0-304a-4d24-f072-6f92d27609ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence for sentiment analysis: She is Amazing\n",
            "\n",
            "--- SENTIMENT CLASSIFICATION RESULT ---\n",
            "Input Text: She is Amazing\n",
            "Positive words found: 1\n",
            "Negative words found: 0\n",
            "Overall Sentiment: Positive üòä\n"
          ]
        }
      ]
    }
  ]
}